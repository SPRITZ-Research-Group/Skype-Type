#! /usr/bin/python2
import os
import argparse
import time
from multiprocessing import active_children
from dst.libraries.multiplier import multiplier
from multiprocessing import Process, Queue
from dst.output_smart_dictionary import console
from dst.worker import worker
import dst.dispatchers
from threading import Thread
import dst.listeners
from config import *
import json
import matplotlib.pyplot as plt
import itertools
import functools

if __name__ == "__main__":

    #
    # Argument parsing
    #
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description='Runs S&T attack on specified target, using trained pipeline generated by generate_model.py',
        epilog='''\

    S&T works with chains of operators - building blocks that pass data forward.
    Pre-made chains, called opmodes, can be selected with the syntax:

        %(prog)s --opmode MODE --target TARGET --pipeline PIPELINE

    Currently available chains are the following:
        - from_file loads specified wavfile and attacks it with specified pipeline

    Chains can also be specified block by block by setting the respective parameters.
    Data flow between blocks is the following:

        LISTENER --> DISPATCHER --> PIPELINE --> OUTPUT

    where
        - LISTENER is a function that loads audio. Right now only wavfile reader is provided.
        - DISPATCHER is a function that extracts keypress sounds. Right now only an offline dispatcher, that works on
                        a complete audio file (i.e., not a stream) is provided.
        - PIPELINE is a file with a pickled, trained Sklearn pipeline performing feature extraction and classification.
        - OUTPUT shows attack results. Right now only screen output, that prints results on terminal, is provided.
        '''
    )
    # Misc arguments such as version and help
    parser.add_argument('--version', '-v', action='version', version=CONFIG.VERSION)
    # Opmode is a convenience to avoid specifying operator chains
    parser.add_argument("--opmode", choices=['from_file', ],
                        help='Convenience syntax to avoid specifying operator chains')
    parser.add_argument("--target", "-t",nargs='+', type=str,
                        help='Attack target. Valid values depend on the listener')
    # If no opmode is used you can specify operator chains with safe defaults
    parser.add_argument("--listener", "-l", choices=['wavfile', 'input', 'input_interactive'])
    parser.add_argument("--dispatcher", "-d", choices=['offline'])
    # Define the sklearn pipeline to use
    # Multiple pipelines will be allowed - each will receive from dispatcher
    # Watch out - right now only a SINGLE pipeline works
    parser.add_argument("--pipeline", "-p", action='append', type=file, required=True,
                        help='Trained pipeline created by generate_model.py')
    # General options
    parser.add_argument("--workers", "-w", type=int, default=CONFIG.workers,
                        help='Number of workers to dispatch')
    parser.add_argument("--dispatcher_window_size", type=int, default=CONFIG.dispatcher_window_size,
                        help='Window size of keypress samples, in milliseconds')
    parser.add_argument("--dispatcher_threshold", type=int, default=CONFIG.dispatcher_threshold,
                        help='Percentile threshold of keypress sound vs. background noise, [0, 100]')
    parser.add_argument("--dispatcher_min_interval", type=int, default=CONFIG.dispatcher_min_interval,
                        help='Minimum interval between keystrokes, in milliseconds')
    parser.add_argument("--dispatcher_step_size", type=int, default=CONFIG.dispatcher_step_size,
                        help='Scan granularity of dispatchers, in milliseconds')
    parser.add_argument("--dispatcher_persistence", type=int, default=CONFIG.dispatcher_persistence,
                        help='Whether to save mined events')
    parser.add_argument("--n_predictions", "-n", type=int, default=10,
                        help='Number of required predictions for each sample')

    args = parser.parse_args()

    target_files = []
    json_files = []

    def add_file(fl):
        ext = os.path.splitext(fl)[1]
        if ext == '.wav':
            target_files.append(fl)
        if ext == '.json':
            json_files.append(fl)

    # Read training files from specified location
    for f_name in args.target:
        f = os.path.abspath(f_name)
        if os.path.isfile(f):
            add_file(f)
        elif os.path.isdir(f):
            for r, d, fs in os.walk(f):
                for fn in fs:
                    f1 = os.path.abspath(os.path.join(r, fn))
                    add_file(f1)

    #
    # Configuration - update values
    #
    for key in vars(CONFIG).iterkeys():
        if key in vars(args).keys():
            CONFIG.key = args[key]
    for key, val in vars(args).iteritems():
        CONFIG.key = val
    for target in target_files:
        #
        # Main matter
        #
        # Chain elements registration lists
        pipeline_list = []
        # Multipliers and outputs need to be stopped separately
        output_list = []
        multiplier_list = []

        # Convert opmodes to chains first
        if args.opmode == 'from_file':
            args.listener, args.dispatcher = 'wavfile', 'offline'

        # For each chain part, import modules and register them to registration lists
        # First init the listener, remember its output queue
        lq = Queue()
        p = Process(target=getattr(dst.listeners, args.listener), args=(target, lq, CONFIG))
        p.daemon = True
        p.start()

        # Create the required dispatcher
        oq, dq = Queue(), Queue()
        p = Process(target=getattr(dst.dispatchers, args.dispatcher), args=(lq, oq, dq, CONFIG))
        p.daemon = True
        p.start()
        # For each pipeline, create a pool of workers
        for p_idx, pipeline in enumerate(args.pipeline):
            iq, rq = Queue(), Queue()
            for n_worker in xrange(args.workers):
                p = Process(target=worker, args=(pipeline, iq, rq, args.n_predictions, CONFIG))
                p.daemon = True
                p.start()
            pipeline_list.append(iq)
            # Send the output of the pipeline to a terminal, to be displayed
            p = Thread(target=console, args=(rq, target, CONFIG))
            p.daemon = True
            p.start()
            output_list.append((p, rq))
        # Clone dispatcher output to each pipeline input
        p = Process(target=multiplier, args=(oq, [_q for _q in pipeline_list]))
        multiplier_list.append((p, oq))
        p.daemon = True
        p.start()

        #
        # Exit: wait until everyone (except multipliers and outputs who cannot join()
        #
        while len(active_children()) > len(output_list) + len(multiplier_list):
            time.sleep(1)
            pass
        # Wait for user action to join output and terminate
        for _mulp in multiplier_list:
            _mulp[1].put(None)
        for _outp in output_list:
            _outp[1].put(None)
            _outp[0].join()
    for f_name in args.target:
        f = os.path.abspath(f_name)
        if os.path.isfile(f):
            add_file(f)
        elif os.path.isdir(f):
            for r, d, fs in os.walk(f):
                for fn in fs:
                    f1 = os.path.abspath(os.path.join(r, fn))
                    add_file(f1)
    target_predictions = []
    for file_name in json_files:
        file = open(file_name)
        target_predictions.append(json.load(file))
        file.close()
    predictions_flatted = []
    for target in target_predictions:
        for prediction in target:
            predictions_flatted.append(prediction)
    histograms = dict()
    for prediction in predictions_flatted:
        index = prediction[0]
        list_of_keys = prediction[1]
        if index in histograms:
            pass
        else:
            histograms[index] = dict()
        w = 5;
        for key in list_of_keys:
            if key in histograms[index]:
                histograms[index][key] += w
            else:
                histograms[index][key] = w
            if w > 1:
                w -=1
    smart_dictionary_data = [];
    for letter in histograms:
        items = [];
        for key,count in histograms[letter].items():
            items.append((key,count))
        items.sort(key = lambda x:x[1])
        items.reverse()
        smart_dictionary_data.append(items[:5])
        #plotting
        names = list([x[0] for x in items[:10]])
        values = list([x[1] for x in items[:10]])
        fig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)
        axs.bar(names, values)
        fig.suptitle('Categorical Plotting')
        plt.show()
    smart_dict = list(map(lambda tuple:list(tuple) ,list(itertools.product(*smart_dictionary_data))))
    smart_dict = list(map(lambda list:(list,functools.reduce(lambda a,b: a+b[1], list, 0)) ,smart_dict))
    smart_dict.sort(key = lambda guess: guess[1])
    smart_dict.reverse()
    for guess in smart_dict:
        print(''.join((list(map(lambda tuple: tuple[0],guess[0])))))



